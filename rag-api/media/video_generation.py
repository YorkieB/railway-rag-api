"""
Video Generation

Handles video generation, editing, and summarization.
"""
import os
import subprocess
import tempfile
from typing import Optional, Dict, List
from pathlib import Path
from openai import OpenAI
from .storage import media_storage
from cost import CostTracker

# Initialize cost tracker
cost_tracker = CostTracker()

# Check if FFmpeg is available
FFMPEG_AVAILABLE = False
try:
    subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
    FFMPEG_AVAILABLE = True
except (subprocess.CalledProcessError, FileNotFoundError):
    pass


class VideoGenerator:
    """Handles video generation, editing, and summarization"""
    
    def __init__(self):
        """Initialize video generator"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        self.client = OpenAI(api_key=api_key)
    
    def generate_video(
        self,
        prompt: str,
        duration: int = 5,
        resolution: str = "720p",
        fps: int = 24,
        user_id: str = "default"
    ) -> Dict:
        """
        Generate video from text prompt.
        
        Note: OpenAI doesn't have a video generation API yet (as of 2024).
        This is a placeholder for future integration with RunwayML, Pika Labs, or similar.
        
        Args:
            prompt: Text description of the video
            duration: Video duration in seconds
            resolution: Video resolution (720p, 1080p)
            fps: Frames per second
            user_id: User identifier
            
        Returns:
            Dict with video_id, status, and estimated cost
        """
        # Placeholder implementation
        # In production, this would integrate with a video generation API
        # For now, return a placeholder response
        
        # Estimate cost (placeholder: $0.10 per second)
        cost = 0.10 * duration
        cost_tracker.update_budget(user_id, cost, "video_generation")
        
        return {
            "video_id": None,  # Would be generated by video API
            "status": "not_implemented",
            "message": "Video generation API not yet available. Will integrate with RunwayML/Pika Labs when available.",
            "estimated_cost": cost,
            "duration": duration,
            "resolution": resolution,
            "fps": fps
        }
    
    def edit_video(
        self,
        video_path: str,
        operations: List[Dict],
        user_id: str = "default"
    ) -> Dict:
        """
        Edit video using FFmpeg.
        
        Supported operations:
        - trim: {"type": "trim", "start": seconds, "end": seconds}
        - merge: {"type": "merge", "videos": [paths]}
        - speed: {"type": "speed", "factor": float}
        - volume: {"type": "volume", "factor": float}
        - overlay_text: {"type": "overlay_text", "text": str, "position": "top|bottom|center"}
        
        Args:
            video_path: Path to input video
            operations: List of editing operations
            user_id: User identifier
            
        Returns:
            Dict with output_path and cost
        """
        if not FFMPEG_AVAILABLE:
            raise Exception("FFmpeg is not available. Please install FFmpeg to use video editing.")
        
        output_path = tempfile.mktemp(suffix=".mp4")
        
        try:
            # Build FFmpeg command based on operations
            ffmpeg_cmd = ["ffmpeg", "-i", video_path]
            
            for op in operations:
                if op["type"] == "trim":
                    ffmpeg_cmd.extend(["-ss", str(op["start"]), "-t", str(op["end"] - op["start"])])
                elif op["type"] == "speed":
                    ffmpeg_cmd.extend(["-filter:v", f"setpts={1/op['factor']}*PTS"])
                elif op["type"] == "volume":
                    ffmpeg_cmd.extend(["-filter:a", f"volume={op['factor']}"])
                elif op["type"] == "overlay_text":
                    position_map = {
                        "top": "10:10",
                        "bottom": "10:H-th-10",
                        "center": "(W-tw)/2:(H-th)/2"
                    }
                    pos = position_map.get(op.get("position", "top"), "10:10")
                    text = op["text"].replace(":", "\\:").replace("'", "\\'")
                    ffmpeg_cmd.extend([
                        "-vf", f"drawtext=text='{text}':fontsize=24:fontcolor=white:x={pos.split(':')[0]}:y={pos.split(':')[1]}"
                    ])
            
            ffmpeg_cmd.extend(["-y", output_path])
            
            # Execute FFmpeg
            result = subprocess.run(ffmpeg_cmd, capture_output=True, check=True)
            
            # Read output video
            with open(output_path, "rb") as f:
                video_data = f.read()
            
            # Save to storage
            video_id = media_storage.save_video(
                video_data,
                user_id,
                {
                    "operations": operations,
                    "original_path": video_path
                }
            )
            
            # Estimate cost (minimal for local processing)
            cost = 0.01  # $0.01 for editing
            cost_tracker.update_budget(user_id, cost, "video_editing")
            
            return {
                "video_id": video_id,
                "output_path": output_path,
                "cost": cost
            }
            
        except subprocess.CalledProcessError as e:
            raise Exception(f"FFmpeg error: {e.stderr.decode()}")
        except Exception as e:
            raise Exception(f"Video editing failed: {str(e)}")
    
    def summarize_video(
        self,
        video_path: str,
        frame_sample_rate: float = 1.0,  # frames per second to analyze
        user_id: str = "default"
    ) -> Dict:
        """
        Summarize video by analyzing frames with GPT-4 Vision.
        
        Args:
            video_path: Path to video file
            frame_sample_rate: How many frames per second to analyze
            user_id: User identifier
            
        Returns:
            Dict with summary, key_moments, and cost
        """
        if not FFMPEG_AVAILABLE:
            raise Exception("FFmpeg is not available. Please install FFmpeg to use video summarization.")
        
        try:
            # Extract frames using FFmpeg
            temp_dir = tempfile.mkdtemp()
            fps = frame_sample_rate
            
            # Extract frames
            subprocess.run([
                "ffmpeg", "-i", video_path,
                "-vf", f"fps={fps}",
                f"{temp_dir}/frame_%04d.png"
            ], capture_output=True, check=True)
            
            # Analyze frames with GPT-4 Vision
            frame_files = sorted(Path(temp_dir).glob("frame_*.png"))
            frame_analyses = []
            total_tokens = 0
            
            for i, frame_file in enumerate(frame_files[::int(fps)]):  # Sample every second
                import base64
                with open(frame_file, "rb") as f:
                    frame_data = f.read()
                
                frame_base64 = base64.b64encode(frame_data).decode('utf-8')
                
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": f"Describe what's happening in this video frame (timestamp: {i} seconds). Be concise."},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{frame_base64}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=100
                )
                
                analysis = response.choices[0].message.content
                tokens = response.usage.total_tokens if response.usage else 0
                total_tokens += tokens
                
                frame_analyses.append({
                    "timestamp": i,
                    "analysis": analysis
                })
            
            # Generate overall summary
            summary_prompt = f"Based on these frame analyses, provide a concise video summary:\n\n"
            for fa in frame_analyses:
                summary_prompt += f"[{fa['timestamp']}s] {fa['analysis']}\n"
            
            summary_response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "user", "content": summary_prompt}
                ],
                max_tokens=300
            )
            
            summary = summary_response.choices[0].message.content
            total_tokens += summary_response.usage.total_tokens if summary_response.usage else 0
            
            # Estimate cost
            cost = cost_tracker.estimate_cost(total_tokens, "gpt-4o")
            cost_tracker.update_budget(user_id, cost, "video_summarization")
            
            # Cleanup
            import shutil
            shutil.rmtree(temp_dir)
            
            return {
                "summary": summary,
                "key_moments": frame_analyses,
                "frames_analyzed": len(frame_analyses),
                "tokens_used": total_tokens,
                "cost": cost
            }
            
        except Exception as e:
            raise Exception(f"Video summarization failed: {str(e)}")


# Global video generator instance
video_generator = VideoGenerator()

